---
title: "Milestone Report"
author: "Shaofeng Chen"
date: "11/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This report tries to explore the trainning data set used to build a predictive model for automatic text suggestion applications. Natural language processing technique will be applied on top of the trainning data set in order to build the predictive model.

## Summary of data set

We define the following function to get a summary of the data files, including file size, number of lines and number of words:

```{r, echo=TRUE}
summary <- function(dir) {
  library(ngram)
  files <- list.files(dir, full.names = TRUE)
  filenames <- c()
  filesizes <- c()
  lines <- c()
  words <- c()
  for(f in files) {
    filename <- tail(unlist(strsplit(f, "/")), n=1)
    filenames <- append(filenames, filename)
    filesizeMB <- paste(file.info(f)$size / 1024^2, "MB")
    filesizes <- append(filesizes, filesizeMB)
    con <- file(f, "r")
    fileLines <- readLines(con, skipNul = TRUE)
    lines <- append(lines, length(fileLines))
    words <- append(words, wordcount(fileLines))
    close(con)
  }
  data.frame(filename=filenames, size=filesizes, line.count=lines, word.count=words)
}
```

The summary looks like:
```{r, echo=FALSE}
summary("./final/en_US")
```

## Obtain term document

We use the R package 'tm' to obtain the term document from the taining dataset. Since the dataset is big we first use the following function to get a sample of the documents:
```{r, echo=TRUE}
sampleBy <- function(dir, percentage) {
  set.seed(2019)
  files <- list.files(dir, full.names = TRUE)
  result <- c()
  for(f in files) {
    con <- file(f, "r")
    fileLines <- readLines(con, skipNul = TRUE)
    result <- append(result, fileLines)
    close(con)
  }
  sample(result, length(result) * percentage)
}
```

The sample can be obtained by:
```{r, echo=TRUE}
sampledData <- sampleBy("./final/en_US", 0.02)
```

Following two functions are used to create corpus and term document from the input character vectors. In the mean time we clean up the documents by removing URLs and extra spaces:
```{r,echo=TRUE}
toCorpus <- function(characterVectors) {
  library(tm)
  corpus <- VCorpus(VectorSource(characterVectors))
  toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
  corpus <- tm_map(corpus, toSpace, "@[^\\s]+")
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, PlainTextDocument)
  corpus
}

toTermDocumentMatrix <- function(corpus, sparse, n) {
  if (n == 1) {
    return (removeSparseTerms(TermDocumentMatrix(corpus), sparse))
  }
  library(RWeka)
  tdm <- TermDocumentMatrix(corpus, 
                            control = list(tokenize = function(x) NGramTokenizer(x, Weka_control(min = n, max = n))))
  removeSparseTerms(tdm, sparse)
}
```

The one gram, 2-gram, and 3-gram term documents can be obtained by:
```{r, echo=TRUE}
corpus <- toCorpus(sampledData)
tdm <- toTermDocumentMatrix(corpus, 0.9999, 1)
tdm2 <- toTermDocumentMatrix(corpus, 0.9999, 2)
tdm3 <- toTermDocumentMatrix(corpus, 0.9999, 3)
```

## Exploratory Analysis


Given the term document we use the following function to get the frequency of the words:
```{r, echo=TRUE}
sortFreq <- function(tdm, decreasing = FALSE) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = decreasing)
  data.frame(word = names(freq), freq = freq)
}
```

So the freqency of 1-gram, 2-gram and 3-gram words are:
```{r, echo=TRUE}
freq <- sortFreq(tdm, TRUE)
freq2 <- sortFreq(tdm2, TRUE)
freq3 <- sortFreq(tdm3, TRUE)
```

Using the above function, the top most 30 words of the 1-gram word frequency will be:
```{r, echo=FALSE, message=FALSE}
library(ggplot2)
ggplot(freq[1:30, ], aes(x=word, y=freq, fill=word)) + 
  geom_bar(width = 0.75,  stat = "identity", colour = "black", size = 1) + 
  coord_polar(theta = "x") + xlab("") + ylab("") + ggtitle("Top 30 Word Frequency") + 
  theme(legend.position = "none") + labs(x = NULL, y = NULL)
```


The top most 30 2-grams:

```{r, echo=FALSE, message=FALSE}
ggplot(freq2[1:30, ], aes(x=word, y=freq, fill=word)) + 
  geom_bar(width = 0.75,  stat = "identity", colour = "black", size = 1) + 
  coord_polar(theta = "x") + xlab("") + ylab("") + ggtitle("Top 30 2-gram Frequency") + 
  theme(legend.position = "none") + labs(x = NULL, y = NULL)
```


The top most 30 3-grams:

```{r, echo=FALSE, message=FALSE}
ggplot(freq3[1:30, ], aes(x=word, y=freq, fill=word)) + 
  geom_bar(width = 0.75,  stat = "identity", colour = "black", size = 1) + 
  coord_polar(theta = "x") + xlab("") + ylab("") + ggtitle("Top 30 3-gram Frequency") + 
  theme(legend.position = "none") + labs(x = NULL, y = NULL)
```

## Future work

For future work we will build the predictive model based on the frequency of the 1-gram, 2-gram and 3-gram word sequences. 